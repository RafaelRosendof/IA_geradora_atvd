{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "k1wTzHtdv3pz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "from torchvision.utils import save_image\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import time as time\n",
        "import torchvision.transforms as T # Para manipular imagens para o FID\n",
        "import torchvision.datasets as datasets # Para carregar o dataset real para o FID\n",
        "import tempfile # Para criar diretórios temporários para imagens\n",
        "import multiprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-fid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW8q1AHUPwIe",
        "outputId": "a2ac7f62-7971-4d23-cc68-d2888e0b9063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-fid\n",
            "  Downloading pytorch_fid-0.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (11.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.0.1->pytorch-fid)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.0.1->pytorch-fid)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.0.1->pytorch-fid)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.1->pytorch-fid)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.1->pytorch-fid)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.1->pytorch-fid)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.1->pytorch-fid)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.1->pytorch-fid)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.1->pytorch-fid)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.1->pytorch-fid)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.1->pytorch-fid) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.1->pytorch-fid) (3.0.2)\n",
            "Downloading pytorch_fid-0.3.0-py3-none-any.whl (15 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-fid\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-fid-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_fid.fid_score import calculate_fid_given_paths"
      ],
      "metadata": {
        "id": "Wrf3m8A_P3Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DY1kmwuXv4zI",
        "outputId": "61140c49-3bc4-4c35-b977-2ccc495b0d80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/kushsheth/face-vae?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.33G/1.33G [00:15<00:00, 92.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "# Acesso ao diretório de dados do Kaggle, adaptado do seu notebook VAE\n",
        "# Este trecho é específico para ambiente Kaggle. Fora dele, ajuste o caminho.\n",
        "try:\n",
        "    import kagglehub\n",
        "    kushsheth_face_vae_path = kagglehub.dataset_download('kushsheth/face-vae')\n",
        "    print('Data source import complete.')\n",
        "except ImportError:\n",
        "    print(\"kagglehub not found. Assuming local setup for data paths.\")\n",
        "    # Fallback for local development: Adjust these paths if running locally\n",
        "    # For local setup, ensure you download the CelebA dataset and list_attr_celeba.csv\n",
        "    # to these relative paths or specify absolute paths.\n",
        "    kushsheth_face_vae_path = './data/face-vae' # Example local path\n",
        "    if not os.path.exists(kushsheth_face_vae_path):\n",
        "        print(\"Local data path not found. Please ensure CelebA dataset is available at expected location.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohvZno3Hv_A6"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters (Ajustados para GANs e para o problema)\n",
        "batch_size = 64 # Reduzido para testar, 128 também é comum\n",
        "epochs = 25 # Aumentado para melhor convergência\n",
        "latent_dim = 100 # Dimensão padrão para o vetor de ruído z\n",
        "learning_rate_g = 0.0004 # Taxa de aprendizado para o Gerador\n",
        "learning_rate_d = 0.00001 # Taxa de aprendizado para o Discriminador (TTUR: pode ser ajustada, ou mais passos D)\n",
        "image_size = 128\n",
        "channels = 3\n",
        "num_attributes = 5 # Smiling, Male, Blond_Hair, Eyeglasses, Wearing_Hat\n",
        "lambda_gp = 0.1 # Parâmetro para Gradient Penalty (WGAN-GP)\n",
        "d_steps = 5 # Número de passos do Discriminador por passo do Gerador. Em WGAN, 5 é comum, mas vamos começar com 1 para estabilidade inicial.\n",
        "\n",
        "# Definir os atributos-alvo\n",
        "target_attrs = ['Smiling', 'Male', 'Blond_Hair', 'Eyeglasses', 'Wearing_Hat']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XX_BCQHcwP2r"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim, num_attributes, img_channels):\n",
        "        super(Generator, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_attributes = num_attributes\n",
        "\n",
        "        # Camada de embedding para os atributos\n",
        "        # Mapeia o vetor de atributos para uma dimensão maior, que será concatenada ao ruído latente\n",
        "        self.attr_embedding = nn.Sequential(\n",
        "            nn.Linear(self.num_attributes, 128), # Ajuste a dimensão de saída conforme necessário\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        # Inicializa a camada de projeção para o início da rede convolucional\n",
        "        # A entrada será latent_dim + dimensão do embedding dos atributos\n",
        "        self.projection_dim = 512 * 4 * 4 # DCGAN starts with 512 filters at 4x4\n",
        "        self.fc_projection = nn.Linear(self.latent_dim + 128, self.projection_dim)\n",
        "\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            # Input: (latent_dim + num_attr_embedding) -> (512 * 4 * 4)\n",
        "            # Reshape: (512, 4, 4)\n",
        "\n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False), # 4x4 -> 8x8\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False), # 8x8 -> 16x16\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),  # 16x16 -> 32x32\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 32, 4, 2, 1, bias=False),  # 32x32 -> 64x64\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(32, img_channels, 4, 2, 1, bias=False), # 64x64 -> 128x128\n",
        "            nn.Tanh() # Saída da imagem normalizada para [-1, 1]\n",
        "        )\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, z, attrs):\n",
        "        # Converte atributos para embedding\n",
        "        attr_emb = self.attr_embedding(attrs)\n",
        "        # Concatena ruído latente e embedding dos atributos\n",
        "        z_conditioned = torch.cat([z, attr_emb], 1)\n",
        "\n",
        "        # Projeta para o formato do input da primeira camada convolucional\n",
        "        h = self.fc_projection(z_conditioned)\n",
        "        h = F.relu(h) # ADICIONAR ESTA LINHA PARA MAIOR EXPRESSIVIDADE\n",
        "        h = h.view(-1, 512, 4, 4) # Reshape para (N, C, H, W)\n",
        "\n",
        "        return self.main(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhrlSQ6mwYqI"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, num_attributes, img_channels):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.num_attributes = num_attributes\n",
        "\n",
        "        self.attr_embedding = nn.Sequential(\n",
        "            nn.Linear(self.num_attributes, 128),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            # Input: (img_channels, 128, 128)\n",
        "            # >>>>> ADIÇÃO AQUI: Spectral Normalization <<<<<\n",
        "            # Adicione SpectralNorm após cada camada Conv2d no Discriminator, exceto a primeira.\n",
        "            # Isso ajuda a estabilizar o treinamento e pode mitigar o colapso de modo.\n",
        "            nn.Conv2d(img_channels, 32, 4, 2, 1, bias=False), # 128x128 -> 64x64\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.utils.spectral_norm(nn.Conv2d(32, 64, 4, 2, 1, bias=False)), # 64x64 -> 32x32\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.utils.spectral_norm(nn.Conv2d(64, 128, 4, 2, 1, bias=False)), # 32x32 -> 16x16\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.utils.spectral_norm(nn.Conv2d(128, 256, 4, 2, 1, bias=False)), # 16x16 -> 8x8\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.utils.spectral_norm(nn.Conv2d(256, 512, 4, 2, 1, bias=False)), # 8x8 -> 4x4\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "        self.fc_final = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512 * 4 * 4 + 128, 1),\n",
        "            # nn.Sigmoid()\n",
        "        )\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, img, attrs):\n",
        "        attr_emb = self.attr_embedding(attrs)\n",
        "        features = self.feature_extractor(img)\n",
        "        features = features.view(features.size(0), -1)\n",
        "        conditioned_features = torch.cat([features, attr_emb], 1)\n",
        "        return self.fc_final(conditioned_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGpdNFgZwfrx",
        "outputId": "7dc3092f-dbe0-4c4f-93c3-844de8ed8f21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device being used: cuda\n"
          ]
        }
      ],
      "source": [
        "# Inicializa modelos\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Device being used: {device}')\n",
        "\n",
        "netG = Generator(latent_dim, num_attributes, channels).to(device)\n",
        "netD = Discriminator(num_attributes, channels).to(device)\n",
        "\n",
        "# Otimizadores para WGAN-GP: Adam com beta1=0.0 para D\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=learning_rate_d, betas=(0.0, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=learning_rate_g, betas=(0.5, 0.999)) # Beta1 padrão para G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "0yow0ABNwFTA",
        "outputId": "335262a5-dbad-48c9-fb20-7201885bc8ee"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-501502356.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# --- Dataset e DataLoader ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCelebAConditionalDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_attrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattributes_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
          ]
        }
      ],
      "source": [
        "# --- Dataset e DataLoader ---\n",
        "class CelebAConditionalDataset(Dataset):\n",
        "    def __init__(self, img_dir, attr_path, transform=None, target_attrs=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.attributes_df = pd.read_csv(attr_path)\n",
        "        self.transform = transform\n",
        "        self.target_attrs = target_attrs\n",
        "\n",
        "        # Certificar que todas as imagens referenciadas existem\n",
        "        # self.image_files = [f for f in self.attributes_df.iloc[:, 0] if os.path.exists(os.path.join(self.img_dir, f))]\n",
        "        # self.attributes_df = self.attributes_df[self.attributes_df.iloc[:, 0].isin(self.image_files)]\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.attributes_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.attributes_df.iloc[idx, 0]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB') # Garante 3 canais\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Obter e converter os atributos para 0/1\n",
        "        attrs = []\n",
        "        if self.target_attrs:\n",
        "            for attr in self.target_attrs:\n",
        "                # O CelebA usa -1 para 'no' e 1 para 'yes'. Convertemos para 0 e 1.\n",
        "                attrs.append(1 if self.attributes_df.iloc[idx][attr] == 1 else 0)\n",
        "\n",
        "        # Converte a lista de atributos para um tensor float\n",
        "        attrs_tensor = torch.tensor(attrs, dtype=torch.float32)\n",
        "\n",
        "        return image, attrs_tensor\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normaliza para [-1, 1]\n",
        "])\n",
        "\n",
        "# Caminhos dos dados (adaptados para o diretório descompactado do KaggleHub ou local)\n",
        "img_dir = os.path.join(kushsheth_face_vae_path, 'img_align_celeba', 'img_align_celeba')\n",
        "attr_path = os.path.join(kushsheth_face_vae_path, 'list_attr_celeba.csv')\n",
        "\n",
        "# Crie o dataset\n",
        "dataset = CelebAConditionalDataset(\n",
        "    img_dir=img_dir,\n",
        "    attr_path=attr_path,\n",
        "    transform=transform,\n",
        "    target_attrs=target_attrs # Passa a lista de atributos\n",
        ")\n",
        "\n",
        "# Crie o DataLoader\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0, # Usar metade dos cores disponíveis\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Verifica um lote de imagens e atributos\n",
        "batch_images, batch_attrs = next(iter(dataloader))\n",
        "print(f\"Batch de Imagens shape: {batch_images.shape}\") # Ex: [64, 3, 128, 128]\n",
        "print(f\"Batch de Atributos shape: {batch_attrs.shape}\") # Ex: [64, 5]\n",
        "\n",
        "# Plot the first 16 images with their attributes\n",
        "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
        "axes = axes.ravel()\n",
        "for i, ax in enumerate(axes):\n",
        "    img = batch_images[i]\n",
        "    img = img * 0.5 + 0.5 # Denormalize\n",
        "    img = img.permute(1, 2, 0).cpu().numpy()\n",
        "    ax.imshow(img)\n",
        "    attr_labels = \" \".join([f\"{target_attrs[j]}: {'Y' if batch_attrs[i,j] == 1 else 'N'}\" for j in range(num_attributes)])\n",
        "    ax.set_title(attr_labels, fontsize=8)\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Cria diretórios para resultados\n",
        "os.makedirs(\"results/cgan\", exist_ok=True)\n",
        "os.makedirs(\"models/cgan\", exist_ok=True)\n",
        "\n",
        "# Configuração do arquivo de log\n",
        "log_file_path = \"results/cgan/training_log.txt\"\n",
        "log_file = open(log_file_path, \"w\")\n",
        "log_file.write(\"Iniciando o treinamento de CGAN com WGAN-GP\\n\")\n",
        "log_file.write(f\"Hyperparameters: Batch Size={batch_size}, Epochs={epochs}, Latent Dim={latent_dim}\\n\")\n",
        "log_file.write(f\"Learning Rate G={learning_rate_g}, Learning Rate D={learning_rate_d}\\n\")\n",
        "log_file.write(f\"Lambda GP={lambda_gp}, D Steps={d_steps}\\n\")\n",
        "log_file.write(\"-\" * 50 + \"\\n\")\n",
        "\n",
        "# --- Definição do Modelo CGAN (Baseado em DCGAN com Condicionamento) ---\n",
        "\n",
        "# Função para inicializar pesos (melhora estabilidade em GANs)\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5R0mpQ0NwknJ"
      },
      "outputs": [],
      "source": [
        "# --- Função de Perda WGAN-GP ---\n",
        "def calculate_gradient_penalty(discriminator, real_samples, fake_samples, attrs, lambda_gp, device):\n",
        "    \"\"\"Calcula o gradient penalty para WGAN-GP.\"\"\"\n",
        "    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)\n",
        "    # Interpolação linear entre amostras reais e falsas\n",
        "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
        "\n",
        "    # Passa as interpolações e atributos pelo discriminador\n",
        "    d_interpolates = discriminator(interpolates, attrs)\n",
        "\n",
        "    # Calcula gradientes de d_interpolates em relação a interpolates\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=torch.ones_like(d_interpolates),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        allow_unused=True\n",
        "    )[0]\n",
        "\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    gradient_norm = gradients.norm(2, dim=1)\n",
        "    gradient_penalty = ((gradient_norm - 1) ** 2).mean() * lambda_gp\n",
        "    return gradient_penalty\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configurações para FID ---\n",
        "# Caminho para o dataset real (necessário para calcular o FID)\n",
        "# Certifique-se de que este caminho esteja correto para o seu ambiente Colab/Kaggle\n",
        "real_data_root = os.path.join(kushsheth_face_vae_path, 'img_align_celeba', 'img_align_celeba')\n",
        "\n",
        "# Número de amostras para gerar e usar no cálculo do FID (deve ser um número razoável)\n",
        "num_fid_samples = 10000  # Comece com um número menor para testes, pode aumentar para 10000+\n",
        "\n",
        "# Transformação para as imagens que serão salvas para o cálculo do FID (normalmente [0,1] ou [0,255])\n",
        "# O FID espera imagens RGB, normalizadas para [0, 1] ou [0, 255]\n",
        "fid_image_transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(), # Imagens em [0, 1]\n",
        "])\n",
        "\n",
        "# Função auxiliar para gerar e salvar imagens para o cálculo do FID\n",
        "def generate_and_save_images_for_fid(generator, latent_dim, num_attributes, num_images, device, output_path, target_attrs_list):\n",
        "    generator.eval()\n",
        "    imgs_saved = 0\n",
        "    while imgs_saved < num_images:\n",
        "        batch_noise = torch.randn(min(num_images - imgs_saved, batch_size), latent_dim, device=device)\n",
        "        # Gerar atributos aleatórios para a diversidade do FID\n",
        "        batch_attrs = torch.randint(0, 2, (batch_noise.size(0), num_attributes), dtype=torch.float32).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            fake_images = generator(batch_noise, batch_attrs).cpu()\n",
        "\n",
        "        for j in range(fake_images.size(0)):\n",
        "            img = fake_images[j]\n",
        "            img = img * 0.5 + 0.5 # Denormaliza para [0, 1]\n",
        "            save_image(img, os.path.join(output_path, f'generated_fid_{imgs_saved}.png'))\n",
        "            imgs_saved += 1\n",
        "            if imgs_saved >= num_images:\n",
        "                break\n",
        "    generator.train() # Voltar ao modo de treinamento\n",
        "    return output_path\n",
        "\n",
        "# Função auxiliar para preparar as imagens reais para o cálculo do FID\n",
        "# Atenção: Esta função pode ser demorada na primeira execução se o dataset for grande\n",
        "# e precisar ser copiado ou ter suas estatísticas calculadas.\n",
        "def prepare_real_images_for_fid(data_root, num_images, output_path, transform):\n",
        "    if not os.path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "        # Cria um dataset temporário para copiar as imagens reais\n",
        "        # Para CelebA, carregar todas as imagens e copiar é ineficiente.\n",
        "        # O pytorch-fid pode ler diretamente de um diretório.\n",
        "        # O ideal é usar o DataLoader para pegar um subconjunto aleatório de imagens reais.\n",
        "\n",
        "        # Solução mais simples e comum com pytorch-fid:\n",
        "        # A biblioteca espera um diretório com imagens.\n",
        "        # Podemos simplesmente usar o real_data_root.\n",
        "        print(f\"Usando imagens reais de: {data_root}\")\n",
        "    return data_root\n",
        "\n",
        "# Lista para armazenar os valores de FID\n",
        "fid_scores_per_epoch = []"
      ],
      "metadata": {
        "id": "rmcdzx9AOyys"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate_g = 0.0004\n",
        "learning_rate_d = 0.00005\n",
        "d_steps = 5\n",
        "lambda_gp = 10.0"
      ],
      "metadata": {
        "id": "otlm4NPTR8Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQoEeA1Yvu8y",
        "outputId": "9a29ae9f-669e-4447-a2dc-d951ce0477d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando o treinamento...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/25:  33%|███▎      | 1060/3166 [03:17<07:13,  4.86it/s, Loss D=-17.1699, Loss G=8.0363]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7eb30078e0c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7eb30078e0c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Epoch 1/25: 100%|██████████| 3166/3166 [09:49<00:00,  5.37it/s, Loss D=-15.0935, Loss G=24.1256]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Epoch 1/25 Concluída ---\n",
            "  Avg Loss D: -15.0282 | Avg Loss G: 19.2406\n",
            "  Duração da Época: 0h 9m 49s\n",
            "  Tempo Total Decorrido: 0h 9m 49s\n",
            "  Tempo Estimado Restante: 3h 55m 40s\n",
            "  Progresso Total: 4.00%\n",
            "  >>> Modelo salvo! Nova melhor G Loss: 19.2406 na Época 1 <<<\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/25:  34%|███▎      | 1061/3166 [03:15<06:28,  5.41it/s, Loss D=-14.4201, Loss G=28.7340]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7eb30078e0c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7eb30078e0c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Epoch 2/25: 100%|██████████| 3166/3166 [09:46<00:00,  5.40it/s, Loss D=-25.5111, Loss G=17.2772]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Epoch 2/25 Concluída ---\n",
            "  Avg Loss D: -12.1464 | Avg Loss G: 21.4268\n",
            "  Duração da Época: 0h 9m 46s\n",
            "  Tempo Total Decorrido: 0h 19m 35s\n",
            "  Tempo Estimado Restante: 3h 45m 21s\n",
            "  Progresso Total: 8.00%\n",
            "  >>> Modelo salvo! Nova melhor G Loss: 21.4268 na Época 2 <<<\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/25:  33%|███▎      | 1053/3166 [03:15<05:57,  5.91it/s, Loss D=-6.7507, Loss G=-1.7386]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7eb30078e0c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7eb30078e0c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Epoch 3/25: 100%|██████████| 3166/3166 [09:48<00:00,  5.38it/s, Loss D=-23.2977, Loss G=51.7374]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Epoch 3/25 Concluída ---\n",
            "  Avg Loss D: -12.1861 | Avg Loss G: 25.9041\n",
            "  Duração da Época: 0h 9m 48s\n",
            "  Tempo Total Decorrido: 0h 29m 25s\n",
            "  Tempo Estimado Restante: 3h 35m 44s\n",
            "  Progresso Total: 12.00%\n",
            "  >>> Modelo salvo! Nova melhor G Loss: 25.9041 na Época 3 <<<\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/25:  33%|███▎      | 1045/3166 [03:13<06:07,  5.77it/s, Loss D=-9.2342, Loss G=63.9261]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7eb30078e0c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7eb30078e0c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Epoch 4/25: 100%|██████████| 3166/3166 [09:44<00:00,  5.41it/s, Loss D=-6.6853, Loss G=4.4843]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Epoch 4/25 Concluída ---\n",
            "  Avg Loss D: -12.4176 | Avg Loss G: 30.8002\n",
            "  Duração da Época: 0h 9m 45s\n",
            "  Tempo Total Decorrido: 0h 39m 10s\n",
            "  Tempo Estimado Restante: 3h 25m 40s\n",
            "  Progresso Total: 16.00%\n",
            "  >>> Modelo salvo! Nova melhor G Loss: 30.8002 na Época 4 <<<\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/25:  33%|███▎      | 1045/3166 [03:14<08:05,  4.36it/s, Loss D=-7.9749, Loss G=34.6193]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7eb30078e0c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7eb30078e0c0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Epoch 5/25:  38%|███▊      | 1209/3166 [03:46<05:22,  6.07it/s, Loss D=-6.6927, Loss G=35.2471]"
          ]
        }
      ],
      "source": [
        "# --- Treinamento ---\n",
        "print(\"Iniciando o treinamento...\")\n",
        "\n",
        "fixed_noise = torch.randn(64, latent_dim, device=device)\n",
        "fixed_attrs_base = torch.tensor([\n",
        "    [1, 0, 0, 0, 0], # Smiling Female Dark NoGlasses NoHat\n",
        "    [0, 1, 0, 0, 0], # NotSmiling Male Dark NoGlasses NoHat\n",
        "    [1, 0, 1, 0, 0], # Smiling Female Blond NoGlasses NoHat\n",
        "    [0, 1, 1, 0, 0], # NotSmiling Male Blond NoGlasses NoHat\n",
        "    [1, 0, 0, 1, 0], # Smiling Female Dark Glasses NoHat\n",
        "    [0, 1, 0, 1, 0], # NotSmiling Male Dark Glasses NoHat\n",
        "    [1, 0, 0, 0, 1], # Smiling Female Dark NoGlasses Hat\n",
        "    [0, 1, 0, 0, 1]  # NotSmiling Male Dark NoGlasses Hat\n",
        "], dtype=torch.float32).to(device)\n",
        "\n",
        "fixed_attrs = fixed_attrs_base.repeat(8, 1)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "g_losses_per_epoch = []\n",
        "d_losses_per_epoch = []\n",
        "\n",
        "best_g_loss = float('-inf') # Queremos a Loss G mais próxima de zero (menos negativa), então inicializamos com -infinito\n",
        "best_epoch = 0\n",
        "\n",
        "# >>> ADICIONADO PARA FID <<<\n",
        "# Crie um diretório temporário para as imagens geradas para FID\n",
        "temp_gen_fid_dir = tempfile.mkdtemp(prefix='fid_gen_')\n",
        "# Use o diretório de dados reais diretamente, ou crie um subconjunto se preferir\n",
        "real_fid_dir = real_data_root # Ou crie um subconjunto como em prepare_real_images_for_fid\n",
        "\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch}/{epochs}\")\n",
        "\n",
        "    current_d_loss_sum = 0\n",
        "    current_g_loss_sum = 0\n",
        "\n",
        "    # Adicionado um contador para os passos de G, para cálculo preciso da média\n",
        "    g_steps_count = 0\n",
        "\n",
        "    for i, (real_images, attrs) in pbar:\n",
        "        log_file.write(f\"Epoch {epoch}/{epochs}, Batch {i}: real_images min: {real_images.min().item():.4f}, max: {real_images.max().item():.4f}\\n\")\n",
        "        real_images = real_images.to(device)\n",
        "        attrs = attrs.to(device)\n",
        "        batch_size = real_images.size(0)\n",
        "\n",
        "        # Treinar o Discriminador\n",
        "        netD.zero_grad()\n",
        "        real_output = netD(real_images, attrs).view(-1)\n",
        "        noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "        fake_images = netG(noise, attrs).detach()\n",
        "        fake_output = netD(fake_images, attrs).view(-1)\n",
        "        gp = calculate_gradient_penalty(netD, real_images.data, fake_images.data, attrs, lambda_gp, device)\n",
        "        log_file.write(f\"DEBUG: Iteration {i}, Gradient Penalty (GP): {gp.item():.4f}\")\n",
        "        d_loss = -torch.mean(real_output) + torch.mean(fake_output) + gp\n",
        "        d_loss.backward()\n",
        "        optimizerD.step()\n",
        "        current_d_loss_sum += d_loss.item()\n",
        "\n",
        "        # Treinar o Gerador (a cada d_steps do Discriminador)\n",
        "        if i % d_steps == 0:\n",
        "          netG.zero_grad()\n",
        "          noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "          fake_images = netG(noise, attrs)\n",
        "\n",
        "          # DEBUG CRÍTICO: Inspecionar a saída do Discriminador para imagens falsas\n",
        "          g_output = netD(fake_images, attrs).view(-1)\n",
        "          log_file.write(f\"DEBUG: Iteration {i}, G_Output (Discriminator on Fakes) Sample: {g_output[0].item():.4f}, Min: {g_output.min().item():.4f}, Max: {g_output.max().item():.4f}, Mean: {g_output.mean().item():.4f}\")\n",
        "\n",
        "          g_loss = -torch.mean(g_output)\n",
        "          log_file.write(f\"DEBUG: Iteration {i}, G_Loss calculated: {g_loss.item():.4f}\")\n",
        "\n",
        "          g_loss.backward()\n",
        "          # DEBUG CRÍTICO: Inspecionar gradientes do Gerador ANTES do optimizer.step()\n",
        "          for name, param in netG.named_parameters():\n",
        "              if param.grad is not None:\n",
        "                  if (param.grad != 0).any(): # Verifica se há gradientes diferentes de zero\n",
        "                      log_file.write(f\"DEBUG: G Gradient for {name} (norm): {param.grad.norm().item():.6f}\")\n",
        "                      if param.grad.norm().item() > 1e6: # Gradiente explodindo\n",
        "                          log_file.write(f\"WARNING: Exploding gradient detected for {name} in Generator!\")\n",
        "                      elif param.grad.norm().item() < 1e-8: # Gradiente sumindo\n",
        "                          log_file.write(f\"WARNING: Vanishing gradient detected for {name} in Generator!\")\n",
        "              else:\n",
        "                log_file.write(f\"DEBUG: G Gradient for {name} is None (No gradient)\") # Não deve acontecer se loss.backward() é chamado\n",
        "\n",
        "          optimizerG.step()\n",
        "          current_g_loss_sum += g_loss.item()\n",
        "          g_steps_count += 1\n",
        "\n",
        "        pbar.set_postfix({'Loss D': f'{d_loss.item():.4f}', 'Loss G': f'{g_loss.item():.4f}'})\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_duration = epoch_end_time - epoch_start_time\n",
        "\n",
        "    avg_d_loss = current_d_loss_sum / len(dataloader)\n",
        "    # Usar g_steps_count para calcular a média da perda do Gerador\n",
        "    avg_g_loss = current_g_loss_sum / g_steps_count if g_steps_count > 0 else 0\n",
        "\n",
        "    g_losses_per_epoch.append(avg_g_loss)\n",
        "    d_losses_per_epoch.append(avg_d_loss)\n",
        "\n",
        "    total_time_elapsed = epoch_end_time - start_time\n",
        "    avg_time_per_epoch = total_time_elapsed / epoch\n",
        "    estimated_time_remaining = avg_time_per_epoch * (epochs - epoch)\n",
        "\n",
        "    def format_time(seconds):\n",
        "        m, s = divmod(seconds, 60)\n",
        "        h, m = divmod(m, 60)\n",
        "        return f\"{int(h)}h {int(m)}m {int(s)}s\"\n",
        "\n",
        "    print(f\"--- Epoch {epoch}/{epochs} Concluída ---\")\n",
        "    print(f\"  Avg Loss D: {avg_d_loss:.4f} | Avg Loss G: {avg_g_loss:.4f}\")\n",
        "    print(f\"  Duração da Época: {format_time(epoch_duration)}\")\n",
        "    print(f\"  Tempo Total Decorrido: {format_time(total_time_elapsed)}\")\n",
        "    print(f\"  Tempo Estimado Restante: {format_time(estimated_time_remaining)}\")\n",
        "    print(f\"  Progresso Total: {(epoch/epochs)*100:.2f}%\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_samples = netG(fixed_noise, fixed_attrs).cpu()\n",
        "        save_image(generated_samples, f'results/cgan/epoch_{epoch:03d}_generated_samples.png', nrow=8, normalize=True)\n",
        "\n",
        "    # Calcular FID a cada X épocas (ex: a cada 5 épocas) para economizar tempo\n",
        "    if epoch % 5 == 0: # Ajuste a frequência de cálculo do FID\n",
        "        print(\"\\nCalculando FID...\")\n",
        "        # Gerar e salvar imagens para o FID\n",
        "        gen_path = generate_and_save_images_for_fid(netG, latent_dim, num_attributes, num_fid_samples, device, temp_gen_fid_dir, target_attrs)\n",
        "\n",
        "        # Calcular FID\n",
        "        # Certifique-se que o modelo Inception-v3 é baixado pela biblioteca (primeira vez)\n",
        "        try:\n",
        "            fid_value = calculate_fid_given_paths([real_fid_dir, gen_path],\n",
        "                                                batch_size,\n",
        "                                                device,\n",
        "                                                dims=2048) # Dimensão das features do Inception\n",
        "            fid_scores_per_epoch.append(fid_value)\n",
        "            print(f\"  FID Score na Época {epoch}: {fid_value:.2f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao calcular FID na Época {epoch}: {e}\")\n",
        "            fid_scores_per_epoch.append(float('nan')) # Adicionar NaN se houver erro\n",
        "\n",
        "        # Limpar diretório temporário para a próxima avaliação FID\n",
        "        import shutil\n",
        "        shutil.rmtree(temp_gen_fid_dir)\n",
        "        temp_gen_fid_dir = tempfile.mkdtemp(prefix='fid_gen_') # Recria o diretório\n",
        "\n",
        "    # Salvar o melhor modelo (lógica baseada em G Loss)\n",
        "    if avg_g_loss > best_g_loss:\n",
        "        best_g_loss = avg_g_loss\n",
        "        best_epoch = epoch\n",
        "        torch.save(netG.state_dict(), 'models/cgan/best_generator.pth')\n",
        "        torch.save(netD.state_dict(), 'models/cgan/best_discriminator.pth')\n",
        "        print(f\"  >>> Modelo salvo! Nova melhor G Loss: {best_g_loss:.4f} na Época {best_epoch} <<<\")\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        torch.save(netG.state_dict(), f'models/cgan/netG_epoch_{epoch:03d}.pth')\n",
        "        torch.save(netD.state_dict(), f'models/cgan/netD_epoch_{epoch:03d}.pth')\n",
        "\n",
        "\n",
        "print(\"\\nTreinamento concluído.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "💹 *gráficos*"
      ],
      "metadata": {
        "id": "9twwaak3H9gy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxDAVJJH2u-S"
      },
      "outputs": [],
      "source": [
        "total_training_duration = time.time() - start_time\n",
        "print(f\"Duração total do treinamento: {format_time(total_training_duration)}\")\n",
        "\n",
        "# Opcional: Plotar as perdas ao longo do tempo para visualização de progresso\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(d_losses_per_epoch, label='Discriminator Loss')\n",
        "plt.plot(g_losses_per_epoch, label='Generator Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('GAN Training Losses Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('results/cgan/training_losses.png')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "teste *dgfdfd*"
      ],
      "metadata": {
        "id": "RzDpgbYJN6UV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4H-6Dz4f1RK"
      },
      "outputs": [],
      "source": [
        "# prompt: ## generate image using model\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Generate an image\n",
        "def generate_image(generator, noise, attrs, device):\n",
        "    \"\"\"Generates an image using the trained generator.\"\"\"\n",
        "    generator.eval()  # Set generator to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        noise = noise.to(device)\n",
        "        attrs = attrs.to(device)\n",
        "        generated_image = generator(noise, attrs)\n",
        "        # Denormalize image\n",
        "        generated_image = generated_image * 0.5 + 0.5\n",
        "    generator.train() # Set generator back to training mode\n",
        "    return generated_image.cpu() # Return to CPU\n",
        "\n",
        "# Example usage: Generate one image with specific attributes\n",
        "# Create a single noise vector\n",
        "single_noise = torch.randn(1, latent_dim, device=device)\n",
        "\n",
        "# Create a single attribute vector (e.g., Smiling, Male, Blond_Hair, No Eyeglasses, No Hat)\n",
        "# Ensure the order matches target_attrs: ['Smiling', 'Male', 'Blond_Hair', 'Eyeglasses', 'Wearing_Hat']\n",
        "desired_attrs = torch.tensor([[1, 1, 1, 0, 0]], dtype=torch.float32).to(device) # Example: Smiling Male Blond\n",
        "\n",
        "# Generate the image\n",
        "generated_single_image = generate_image(netG, single_noise, desired_attrs, device)\n",
        "\n",
        "# Save or display the generated image\n",
        "save_image(generated_single_image, 'results/cgan/single_generated_image.png')\n",
        "\n",
        "# Display the image (optional)\n",
        "plt.imshow(generated_single_image.squeeze().permute(1, 2, 0).numpy())\n",
        "plt.title(\"Generated Image with Attributes: Smiling, Male, Blond Hair\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}