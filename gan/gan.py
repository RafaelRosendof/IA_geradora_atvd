import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import matplotlib.pyplot as plt
import pandas as pd
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Dataset
from PIL import Image
from torchvision.utils import save_image
from tqdm import tqdm
import numpy as np
import time as time
import torchvision.transforms as T
import torchvision.datasets as datasets
import tempfile
import shutil
import argparse

# Importar a fun√ß√£o calculate_fid_given_paths do pytorch_fid
from pytorch_fid.fid_score import calculate_fid_given_paths

# Fun√ß√£o para verificar imagens corrompidas no dataset
def scan_corrupted_images(img_dir, attr_path, max_check=None):
    """
    Scan the dataset for corrupted images that cannot be loaded.
    
    Args:
        img_dir: Directory containing images
        attr_path: Path to attributes CSV file
        max_check: Maximum number of images to check (None for all)
    
    Returns:
        List of corrupted image paths
    """
    print("Scanning dataset for corrupted images...")
    attributes_df = pd.read_csv(attr_path)
    corrupted_files = []
    
    total_files = len(attributes_df) if max_check is None else min(max_check, len(attributes_df))
    
    for idx in range(total_files):
        img_name = attributes_df.iloc[idx, 0]
        img_path = os.path.join(img_dir, img_name)
        
        try:
            with Image.open(img_path) as img:
                img.convert('RGB')
                img.load()  # Force load to catch any corruption
        except (OSError, IOError, Image.UnidentifiedImageError) as e:
            print(f"Corrupted image found: {img_name} - {e}")
            corrupted_files.append(img_path)
        
        if idx % 1000 == 0:
            print(f"Checked {idx}/{total_files} images... Found {len(corrupted_files)} corrupted so far.")
    
    print(f"Scan complete. Found {len(corrupted_files)} corrupted images out of {total_files} checked.")
    return corrupted_files

# Acesso ao diret√≥rio de dados do CelebA.
# Por favor, ajuste este caminho para onde o dataset CelebA foi descompactado.
# Ele deve apontar para a pasta que cont√©m 'img_align_celeba' e 'list_attr_celeba.csv'.
celeba_dataset_path = './data/celeba' # Exemplo de caminho local. AJUSTE O SEU CAMINHO AQUI!

if not os.path.exists(celeba_dataset_path):
    print(f"ATEN√á√ÉO: O diret√≥rio de dados '{celeba_dataset_path}' n√£o foi encontrado.")
    print("Por favor, baixe o dataset CelebA e ajuste 'celeba_dataset_path' para o local correto.")
    print("O script tentar√° continuar, mas falhar√° sem os dados de imagem/atributos.")

# Hyperparameters (OTIMIZADOS para melhor aprendizado de detalhes)
batch_size = 32 
epochs = 100
latent_dim = 100 
# CORRIGINDO: Learning rates muito baixos impedem aprendizado de detalhes
learning_rate_g = 0.0002  # Aumentado de 0.000005 - CR√çTICO para detalhes
learning_rate_d = 0.0001  # Discriminador mais devagar que gerador
image_size = 128 
channels = 3 
num_attributes = 5 
lambda_gp = 5.0  # Reduzido de 10.0 - permite mais liberdade ao gerador
d_steps = 2      # Reduzido de 5 - balancear G vs D 

# Definir os atributos-alvo
target_attrs = ['Smiling', 'Male', 'Blond_Hair', 'Eyeglasses', 'Wearing_Hat']

# Weight initialization function for GANs (DCGAN paper guidelines)
def weights_init(m):
    """Initialize weights for Conv and BatchNorm layers following DCGAN paper"""
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)

class Generator(nn.Module):
    def __init__(self, latent_dim, num_attributes, img_channels):
        super(Generator, self).__init__()
        self.latent_dim = latent_dim
        self.num_attributes = num_attributes

        self.attr_embedding = nn.Sequential(
            nn.Linear(self.num_attributes, 256),  # Aumentado de 128 para 256
            nn.LeakyReLU(0.2, True),             # LeakyReLU em vez de ReLU
            nn.Linear(256, 256),                 # Camada adicional para melhor representa√ß√£o
            nn.LeakyReLU(0.2, True)
        )

        self.projection_dim = 512 * 4 * 4
        self.fc_projection = nn.Sequential(
            nn.Linear(self.latent_dim + 256, self.projection_dim),  # 256 em vez de 128
            nn.BatchNorm1d(self.projection_dim),                   # BatchNorm adicionado
            nn.ReLU(True)
        )

        self.main = nn.Sequential(
            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False), # 4x4 -> 8x8
            nn.BatchNorm2d(256),
            nn.ReLU(True),

            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False), # 8x8 -> 16x16
            nn.BatchNorm2d(128),
            nn.ReLU(True),

            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False), # 16x16 -> 32x32
            nn.BatchNorm2d(64),
            nn.ReLU(True),

            nn.ConvTranspose2d(64, 32, 4, 2, 1, bias=False), # 32x32 -> 64x64
            nn.BatchNorm2d(32),
            nn.ReLU(True),

            nn.ConvTranspose2d(32, img_channels, 4, 2, 1, bias=False), # 64x64 -> 128x128
            nn.Tanh() # Tanh para sa√≠da de imagem normalizada para [-1, 1]
        )
        self.apply(weights_init)

    def forward(self, z, attrs):
        attr_emb = self.attr_embedding(attrs)
        z_conditioned = torch.cat([z, attr_emb], 1)

        h = self.fc_projection(z_conditioned)
        h = h.view(-1, 512, 4, 4)

        return self.main(h)

class Discriminator(nn.Module):
    def __init__(self, num_attributes, img_channels):
        super(Discriminator, self).__init__()
        self.num_attributes = num_attributes

        self.attr_embedding = nn.Sequential(
            nn.Linear(self.num_attributes, 256),  # Aumentado para 256
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(256, 256),                  # Camada adicional
            nn.LeakyReLU(0.2, inplace=True)
        )

        self.feature_extractor = nn.Sequential(
            nn.Conv2d(img_channels, 32, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),

            nn.utils.spectral_norm(nn.Conv2d(32, 64, 4, 2, 1, bias=False)),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2, inplace=True),

            nn.utils.spectral_norm(nn.Conv2d(64, 128, 4, 2, 1, bias=False)),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),

            nn.utils.spectral_norm(nn.Conv2d(128, 256, 4, 2, 1, bias=False)),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),

            nn.utils.spectral_norm(nn.Conv2d(256, 512, 4, 2, 1, bias=False)),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
        )

        self.fc_final = nn.Sequential(
            nn.Flatten(),
            nn.Linear(512 * 4 * 4 + 256, 512),  # Camada intermedi√°ria
            nn.LeakyReLU(0.2, True),
            nn.Dropout(0.3),                     # Dropout para regulariza√ß√£o
            nn.Linear(512, 1),
            # nn.Sigmoid() # Removido para WGAN-GP
        )
        self.apply(weights_init)

    def forward(self, img, attrs):
        attr_emb = self.attr_embedding(attrs)
        features = self.feature_extractor(img)
        features = features.view(features.size(0), -1)
        conditioned_features = torch.cat([features, attr_emb], 1)
        return self.fc_final(conditioned_features)

# Inicializa modelos
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f'Device being used: {device}')

if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True 
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True 
    torch.backends.cudnn.deterministic = False 
    torch.backends.cudnn.enabled = True 
    print(f'GPU Name: {torch.cuda.get_device_name(0)}')
    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')
    
    torch.cuda.empty_cache()
    print(f'CUDA Version: {torch.version.cuda}')
    print(f'cuDNN Version: {torch.backends.cudnn.version()}')

netG = Generator(latent_dim, num_attributes, channels).to(device)
netD = Discriminator(num_attributes, channels).to(device)

print("‚ö†Ô∏è torch.compile desabilitado para compatibilidade. Usando modelos padr√£o.")

# ***** AQUI: SCALER COMENTADO PARA FOR√áAR FP32 E DEPURAR NaNs *****
# Usa mixed precision (AMP) para melhor utiliza√ß√£o da GPU e menor consumo de VRAM
# Para depurar NaNs, √© recomendado desabilitar temporariamente a mixed precision.
scaler = None # Comentei 'torch.amp.GradScaler('cuda')' para for√ßar FP32.

def print_gpu_memory_usage():
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        cached = torch.cuda.memory_reserved() / 1024**3
        max_allocated = torch.cuda.max_memory_allocated() / 1024**3
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"GPU Memory - Allocated: {allocated:.2f}GB, Cached: {cached:.2f}GB, Max: {max_allocated:.2f}GB, Total: {total:.2f}GB")
        return allocated, cached, max_allocated, total
    return 0, 0, 0, 0

# Otimizadores para WGAN-GP: Adam com beta1=0.0 para o Discriminador √© CRUCIAL para estabilidade
optimizerD = optim.Adam(netD.parameters(), lr=learning_rate_d, betas=(0.0, 0.999))
optimizerG = optim.Adam(netG.parameters(), lr=learning_rate_g, betas=(0.5, 0.999))

# Learning rate schedulers para converg√™ncia est√°vel
schedulerD = optim.lr_scheduler.CosineAnnealingLR(optimizerD, T_max=epochs, eta_min=1e-6)
schedulerG = optim.lr_scheduler.CosineAnnealingLR(optimizerG, T_max=epochs, eta_min=1e-6)

class CelebAConditionalDataset(Dataset):
    def __init__(self, img_dir, attr_path, transform=None, target_attrs=None):
        self.img_dir = img_dir
        self.attributes_df = pd.read_csv(attr_path)
        self.transform = transform
        self.target_attrs = target_attrs

    def __len__(self):
        return len(self.attributes_df)

    def __getitem__(self, idx):
        max_retries = 10
        for attempt in range(max_retries):
            try:
                current_idx = (idx + attempt) % len(self.attributes_df)
                img_name = self.attributes_df.iloc[current_idx, 0]
                img_path = os.path.join(self.img_dir, img_name)
                
                # Try to open and convert the image
                image = Image.open(img_path).convert('RGB')
                
                # Verify the image can be loaded properly
                image.load()
                
                if self.transform:
                    image = self.transform(image)

                attrs = []
                if self.target_attrs:
                    for attr in self.target_attrs:
                        attrs.append(1 if self.attributes_df.iloc[current_idx][attr] == 1 else 0)

                attrs_tensor = torch.tensor(attrs, dtype=torch.float32)

                return image, attrs_tensor
                
            except (OSError, IOError, Image.UnidentifiedImageError, Exception) as e:
                print(f"Warning: Corrupted image at index {current_idx}, file: {img_name if 'img_name' in locals() else 'unknown'}. Error: {e}")
                print(f"Skipping to next image... (attempt {attempt + 1}/{max_retries})")
                continue
        
        # If we've exhausted all retries, raise an error
        raise RuntimeError(f"Could not load a valid image after {max_retries} attempts starting from index {idx}")

transform = transforms.Compose([
    transforms.Resize(image_size),
    transforms.CenterCrop(image_size),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normaliza para [-1, 1]
])

img_dir = os.path.join(celeba_dataset_path, 'img_align_celeba', 'img_align_celeba')
attr_path = os.path.join(celeba_dataset_path, 'list_attr_celeba.csv')

# Optional: Scan for corrupted images (set to True to enable)
scan_for_corrupted = False
if scan_for_corrupted:
    corrupted_files = scan_corrupted_images(img_dir, attr_path, max_check=1000)  # Check first 1000 images
    if corrupted_files:
        print(f"Warning: Found {len(corrupted_files)} corrupted images. Training will skip these automatically.")

dataset = CelebAConditionalDataset(
    img_dir=img_dir,
    attr_path=attr_path,
    transform=transform,
    target_attrs=target_attrs
)

dataloader = torch.utils.data.DataLoader(
    dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=0,
    pin_memory=True,
    persistent_workers=False
)

os.makedirs("results/cgan_optimized", exist_ok=True)
os.makedirs("models/cgan_optimized", exist_ok=True)

log_file_path = "results/cgan_optimized/training_log.txt"
log_file = open(log_file_path, "w")
log_file.write("Iniciando o treinamento de CGAN com WGAN-GP OTIMIZADO\n")
log_file.write(f"Hyperparameters: Batch Size={batch_size}, Epochs={epochs}, Latent Dim={latent_dim}\n")
log_file.write(f"Learning Rate G={learning_rate_g}, Learning Rate D={learning_rate_d}\n")
log_file.write(f"Lambda GP={lambda_gp}, D Steps={d_steps}\n")
log_file.write("-" * 50 + "\n")

# Fun√ß√µes para checkpoint
def save_checkpoint(epoch, netG, netD, optimizerG, optimizerD, g_losses, d_losses, best_fid, checkpoint_dir='checkpoints'):
    """
    Salva um checkpoint completo do treinamento.
    
    Args:
        epoch: √âpoca atual
        netG: Modelo Generator
        netD: Modelo Discriminator  
        optimizerG: Otimizador do Generator
        optimizerD: Otimizador do Discriminator
        g_losses: Lista de perdas do Generator
        d_losses: Lista de perdas do Discriminator
        best_fid: Melhor FID score at√© agora
        checkpoint_dir: Diret√≥rio para salvar checkpoints
    """
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    checkpoint = {
        'epoch': epoch,
        'netG_state_dict': netG.state_dict(),
        'netD_state_dict': netD.state_dict(),
        'optimizerG_state_dict': optimizerG.state_dict(),
        'optimizerD_state_dict': optimizerD.state_dict(),
        'g_losses': g_losses,
        'd_losses': d_losses,
        'best_fid': best_fid,
        'hyperparameters': {
            'batch_size': batch_size,
            'latent_dim': latent_dim,
            'learning_rate_g': learning_rate_g,
            'learning_rate_d': learning_rate_d,
            'image_size': image_size,
            'channels': channels,
            'num_attributes': num_attributes
        }
    }
    
    # Salva o checkpoint mais recente
    checkpoint_path = os.path.join(checkpoint_dir, 'latest_checkpoint.pth')
    torch.save(checkpoint, checkpoint_path)
    
    # Salva checkpoint a cada 10 √©pocas
    if epoch % 10 == 0:
        epoch_checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch:03d}.pth')
        torch.save(checkpoint, epoch_checkpoint_path)
    
    print(f"Checkpoint salvo: {checkpoint_path}")

def load_checkpoint(checkpoint_path, netG, netD, optimizerG, optimizerD, device):
    """
    Carrega um checkpoint e restaura o estado do treinamento.
    
    Args:
        checkpoint_path: Caminho para o arquivo de checkpoint
        netG: Modelo Generator
        netD: Modelo Discriminator
        optimizerG: Otimizador do Generator
        optimizerD: Otimizador do Discriminator
        device: Device (CPU/GPU)
    
    Returns:
        Tuple com (start_epoch, g_losses, d_losses, best_fid)
    """
    if not os.path.exists(checkpoint_path):
        print(f"Checkpoint n√£o encontrado: {checkpoint_path}")
        return 1, [], [], float('inf')
    
    print(f"Carregando checkpoint: {checkpoint_path}")
    try:
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
        
        # Try to load the models - this might fail due to architecture changes
        try:
            netG.load_state_dict(checkpoint['netG_state_dict'])
            netD.load_state_dict(checkpoint['netD_state_dict'])
            
            # If models loaded successfully, try to load optimizers
            try:
                optimizerG.load_state_dict(checkpoint['optimizerG_state_dict'])
                optimizerD.load_state_dict(checkpoint['optimizerD_state_dict'])
            except Exception as opt_e:
                print(f"‚ö†Ô∏è  Aviso: N√£o foi poss√≠vel carregar os otimizadores: {opt_e}")
                print("Os otimizadores ser√£o reinicializados, mas os modelos foram carregados com sucesso.")
            
            # Restaura as vari√°veis de treinamento
            start_epoch = checkpoint['epoch'] + 1
            g_losses = checkpoint.get('g_losses', [])
            d_losses = checkpoint.get('d_losses', [])
            best_fid = checkpoint.get('best_fid', float('inf'))
            
            print(f"‚úÖ Checkpoint carregado com sucesso! Retomando do epoch {start_epoch}")
            print(f"Melhor FID at√© agora: {best_fid:.4f}")
            
            return start_epoch, g_losses, d_losses, best_fid
            
        except (RuntimeError, KeyError) as model_e:
            print(f"‚ö†Ô∏è  ERRO: Incompatibilidade de arquitetura detectada!")
            print(f"Detalhes: {str(model_e)}")
            print("üîÑ Isso geralmente acontece quando a arquitetura do modelo foi modificada.")
            print("üÜï Iniciando treinamento do ZERO com a nova arquitetura...")
            print("üí° Dica: Os checkpoints antigos ser√£o preservados, mas n√£o s√£o compat√≠veis.")
            return 1, [], [], float('inf')
            
    except Exception as e:
        print(f"‚ùå Erro ao carregar checkpoint: {e}")
        print("üÜï Iniciando treinamento do zero...")
        return 1, [], [], float('inf')

def find_latest_checkpoint(checkpoint_dir='checkpoints'):
    """
    Encontra o checkpoint mais recente no diret√≥rio.
    
    Args:
        checkpoint_dir: Diret√≥rio de checkpoints
    
    Returns:
        Caminho para o checkpoint mais recente ou None se n√£o encontrar
    """
    if not os.path.exists(checkpoint_dir):
        return None
    
    latest_path = os.path.join(checkpoint_dir, 'latest_checkpoint.pth')
    if os.path.exists(latest_path):
        return latest_path
    
    # Procura por checkpoints de √©poca
    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint_epoch_') and f.endswith('.pth')]
    if checkpoint_files:
        # Ordena por n√∫mero da √©poca e retorna o mais recente
        checkpoint_files.sort(key=lambda x: int(x.split('_')[2].split('.')[0]))
        return os.path.join(checkpoint_dir, checkpoint_files[-1])
    
    return None

def list_available_checkpoints(checkpoint_dir='checkpoints'):
    """
    Lista todos os checkpoints dispon√≠veis no diret√≥rio.
    
    Args:
        checkpoint_dir: Diret√≥rio de checkpoints
    
    Returns:
        Lista de dicion√°rios com informa√ß√µes dos checkpoints
    """
    if not os.path.exists(checkpoint_dir):
        print(f"Diret√≥rio de checkpoints n√£o encontrado: {checkpoint_dir}")
        return []
    
    checkpoints = []
    for file in os.listdir(checkpoint_dir):
        if file.endswith('.pth'):
            file_path = os.path.join(checkpoint_dir, file)
            try:
                checkpoint_info = torch.load(file_path, map_location='cpu')
                if 'epoch' in checkpoint_info:
                    checkpoints.append({
                        'file': file,
                        'path': file_path,
                        'epoch': checkpoint_info['epoch'],
                        'best_fid': checkpoint_info.get('best_fid', 'N/A'),
                        'size_mb': os.path.getsize(file_path) / 1024 / 1024,
                        'modified': time.ctime(os.path.getmtime(file_path))
                    })
            except Exception as e:
                print(f"Erro ao ler checkpoint {file}: {e}")
    
    # Ordena por √©poca
    checkpoints.sort(key=lambda x: x['epoch'])
    return checkpoints

def print_checkpoint_info(checkpoint_dir='checkpoints'):
    """
    Imprime informa√ß√µes detalhadas sobre checkpoints dispon√≠veis.
    """
    checkpoints = list_available_checkpoints(checkpoint_dir)
    
    if not checkpoints:
        print("Nenhum checkpoint encontrado.")
        return
    
    print(f"\n=== Checkpoints Dispon√≠veis em '{checkpoint_dir}' ===")
    print(f"{'Arquivo':<25} {'√âpoca':<8} {'Melhor FID':<12} {'Tamanho':<10} {'Modificado'}")
    print("-" * 80)
    
    for cp in checkpoints:
        fid_str = f"{cp['best_fid']:.2f}" if isinstance(cp['best_fid'], float) else str(cp['best_fid'])
        print(f"{cp['file']:<25} {cp['epoch']:<8} {fid_str:<12} {cp['size_mb']:.1f}MB {cp['modified']}")
    
    print("-" * 80)
    print(f"Total: {len(checkpoints)} checkpoints")

if __name__ == '__main__':
    # Argumentos de linha de comando para controle de checkpoints
    parser = argparse.ArgumentParser(description='GAN Training with Checkpoints')
    parser.add_argument('--resume', action='store_true', help='Resume training from latest checkpoint')
    parser.add_argument('--checkpoint-path', type=str, help='Path to specific checkpoint file to load')
    parser.add_argument('--no-checkpoint', action='store_true', help='Start training from scratch, ignoring any existing checkpoints')
    parser.add_argument('--checkpoint-interval', type=int, default=1, help='Save checkpoint every N epochs (default: 1)')
    parser.add_argument('--list-checkpoints', action='store_true', help='List available checkpoints and exit')
    
    args = parser.parse_args()
    
    # Cria o diret√≥rio de checkpoints se n√£o existir
    os.makedirs('checkpoints', exist_ok=True)
    
    # Se solicitado, lista checkpoints e sai
    if args.list_checkpoints:
        print_checkpoint_info('checkpoints')
        exit(0)
    
    # Configura√ß√£o do sistema de checkpoints
    if args.no_checkpoint:
        start_epoch = 1
        g_losses_per_epoch = []
        d_losses_per_epoch = []
        best_fid_score = float('inf')
        print("Iniciando treinamento do zero (ignorando checkpoints existentes).")
    elif args.checkpoint_path:
        if os.path.exists(args.checkpoint_path):
            try:
                start_epoch, g_losses_per_epoch, d_losses_per_epoch, best_fid_score = load_checkpoint(
                    args.checkpoint_path, netG, netD, optimizerG, optimizerD, device)
                print(f"Carregando checkpoint espec√≠fico: {args.checkpoint_path}")
            except RuntimeError as e:
                print(f"‚ö†Ô∏è  ERRO: Incompatibilidade de arquitetura ao carregar checkpoint!")
                print(f"Detalhes: {e}")
                print("üîÑ A arquitetura do modelo foi modificada. Iniciando do zero...")
                start_epoch = 1
                g_losses_per_epoch = []
                d_losses_per_epoch = []
                best_fid_score = float('inf')
        else:
            print(f"Checkpoint especificado n√£o encontrado: {args.checkpoint_path}")
            start_epoch = 1
            g_losses_per_epoch = []
            d_losses_per_epoch = []
            best_fid_score = float('inf')
    elif args.resume or not args.no_checkpoint:
        # Tenta carregar o √∫ltimo checkpoint automaticamente
        latest_checkpoint = find_latest_checkpoint('checkpoints')
        if latest_checkpoint:
            try:
                start_epoch, g_losses_per_epoch, d_losses_per_epoch, best_fid_score = load_checkpoint(
                    latest_checkpoint, netG, netD, optimizerG, optimizerD, device)
                print(f"Retomando treinamento a partir do epoch {start_epoch}...")
            except RuntimeError as e:
                print(f"‚ö†Ô∏è  ERRO: Incompatibilidade de arquitetura ao carregar checkpoint!")
                print(f"Detalhes: {e}")
                print("üîÑ A arquitetura do modelo foi modificada. Iniciando do zero...")
                print("üí° Use --no-checkpoint para pular esta verifica√ß√£o no futuro.")
                start_epoch = 1
                g_losses_per_epoch = []
                d_losses_per_epoch = []
                best_fid_score = float('inf')
        else:
            start_epoch = 1
            g_losses_per_epoch = []
            d_losses_per_epoch = []
            best_fid_score = float('inf')
            print("Nenhum checkpoint encontrado. Iniciando treinamento do zero.")
    else:
        start_epoch = 1
        g_losses_per_epoch = []
        d_losses_per_epoch = []
        best_fid_score = float('inf')
        print("Iniciando treinamento do zero.")
        
    print(f"Configura√ß√£o de checkpoints: Salvar a cada {args.checkpoint_interval} √©pocas")

    # Teste inicial do dataloader e modelos
    if len(dataloader) > 0:
        batch_images, batch_attrs = next(iter(dataloader))
        print(f"Batch de Imagens shape: {batch_images.shape}")
        print(f"Batch de Atributos shape: {batch_attrs.shape}")
        
        print(f"Batch de imagens device: {batch_images.device}")
        test_batch = batch_images.to(device)
        test_attrs = batch_attrs.to(device)
        print(f"Ap√≥s mover para GPU - device: {test_batch.device}")

        print("\n=== Status Inicial da GPU ===")
        print_gpu_memory_usage()
        print("==============================\n")
        
        print("Testando forward pass do Generator...")
        test_noise = torch.randn(4, latent_dim, device=device)
        test_attrs_small = test_attrs[:4]
        with torch.no_grad():
            test_output = netG(test_noise, test_attrs_small)
        print(f"Output do Generator shape: {test_output.shape}")
        print("‚úÖ Forward pass funcionando!")
        
        print("\n=== Status da GPU ap√≥s teste ===")
        print_gpu_memory_usage()
        print("==============================\n")

        fig, axes = plt.subplots(2, 4, figsize=(12, 6))
        axes = axes.ravel()
        for i, ax in enumerate(axes):
            if i < batch_images.shape[0]:
                img = batch_images[i]
                img = img * 0.5 + 0.5
                img = img.permute(1, 2, 0).cpu().numpy()
                ax.imshow(img)
                attr_labels = " ".join([f"{target_attrs[j]}: {'Y' if batch_attrs[i,j] == 1 else 'N'}" for j in range(num_attributes)])
                ax.set_title(attr_labels, fontsize=8)
                ax.axis('off')
        plt.tight_layout()
        plt.show()
    else:
        print("DataLoader est√° vazio. Verifique o caminho dos dados e a configura√ß√£o do dataset.")

# Fun√ß√µes auxiliares para o treinamento
def check_training_stability(d_loss, g_loss, epoch, batch_idx, log_file):
    """
    Check for training instabilities and log warnings.
    
    Args:
        d_loss: Current discriminator loss
        g_loss: Current generator loss
        epoch: Current epoch
        batch_idx: Current batch index
        log_file: Log file handle
    
    Returns:
        bool: True if training appears stable, False otherwise
    """
    instability_detected = False
    
    # Check for extremely high losses
    if abs(d_loss) > 50000:
        warning_msg = f"WARNING: Extremely high D_Loss detected: {d_loss:.4f} at Epoch {epoch}, Batch {batch_idx}"
        print(warning_msg)
        log_file.write(warning_msg + "\n")
        instability_detected = True
    
    if abs(g_loss) > 10000:
        warning_msg = f"WARNING: Extremely high G_Loss detected: {g_loss:.4f} at Epoch {epoch}, Batch {batch_idx}"
        print(warning_msg)
        log_file.write(warning_msg + "\n")
        instability_detected = True
    
    # Check for NaN or infinite values
    if torch.isnan(torch.tensor(d_loss)) or torch.isinf(torch.tensor(d_loss)):
        warning_msg = f"CRITICAL: D_Loss is NaN or Inf at Epoch {epoch}, Batch {batch_idx}"
        print(warning_msg)
        log_file.write(warning_msg + "\n")
        instability_detected = True
    
    if torch.isnan(torch.tensor(g_loss)) or torch.isinf(torch.tensor(g_loss)):
        warning_msg = f"CRITICAL: G_Loss is NaN or Inf at Epoch {epoch}, Batch {batch_idx}"
        print(warning_msg)
        log_file.write(warning_msg + "\n")
        instability_detected = True
    
    return not instability_detected

def adaptive_learning_rate_adjustment(optimizer, current_loss, loss_history, min_lr=1e-6, reduction_factor=0.8):
    """
    Adaptively adjust learning rate based on loss trends.
    
    Args:
        optimizer: The optimizer to adjust
        current_loss: Current loss value
        loss_history: List of recent loss values
        min_lr: Minimum learning rate threshold
        reduction_factor: Factor to reduce learning rate by
    
    Returns:
        bool: True if learning rate was adjusted
    """
    if len(loss_history) < 5:
        return False
    
    # Check if losses are consistently increasing
    recent_losses = loss_history[-5:]
    if all(recent_losses[i] < recent_losses[i+1] for i in range(len(recent_losses)-1)):
        # Losses are consistently increasing, reduce learning rate
        for param_group in optimizer.param_groups:
            old_lr = param_group['lr']
            if old_lr > min_lr:
                param_group['lr'] = max(old_lr * reduction_factor, min_lr)
                print(f"Reduced learning rate from {old_lr:.2e} to {param_group['lr']:.2e}")
                return True
    
    return False

def calculate_gradient_penalty(discriminator, real_samples, fake_samples, attrs, lambda_gp, device):
        """Calcula o gradient penalty para WGAN-GP."""
        batch_size = real_samples.size(0)
        alpha = torch.rand(batch_size, 1, 1, 1, device=device)
        
        # √â CR√çTICO que interpolates tenha requires_grad=True para o c√°lculo do GP
        # real_samples e fake_samples devem ser tensores com hist√≥rico de gradientes (n√£o .data)
        # AQUI: real_samples e fake_samples J√Å DEVEM TER requires_grad=True
        interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples))
        
        # O .requires_grad_(True) √© aplicado AP√ìS a opera√ß√£o para garantir que o hist√≥rico seja rastreado.
        # No entanto, a forma mais robusta √© clonar e aplicar.
        interpolates.requires_grad_(True) 
        
        # discriminator √© chamado aqui. d_interpolates PRECISA de um grad_fn
        d_interpolates = discriminator(interpolates, attrs)

        gradients = torch.autograd.grad(
            outputs=d_interpolates,
            inputs=interpolates,
            grad_outputs=torch.ones_like(d_interpolates, device=device),
            create_graph=True, 
            retain_graph=True,
            only_inputs=True
        )[0]

        gradients = gradients.view(batch_size, -1)
        gradient_norm = gradients.norm(2, dim=1) 
        gradient_penalty = ((gradient_norm - 1) ** 2).mean() * lambda_gp
        return gradient_penalty

# Configura√ß√µes para FID
real_data_root = os.path.join(celeba_dataset_path, 'img_align_celeba', 'img_align_celeba')
num_fid_samples = 2000 

fid_image_transform = transforms.Compose([
    transforms.Resize(image_size),
    transforms.CenterCrop(image_size),
    transforms.ToTensor(),
])

def create_clean_real_images_for_fid(source_dir, output_dir, num_samples):
        """Create a clean dataset for FID calculation by filtering out corrupted images."""
        if os.path.exists(output_dir):
            shutil.rmtree(output_dir)
        os.makedirs(output_dir)
        
        all_images = [f for f in os.listdir(source_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        valid_images = []
        
        print(f"Verificando {len(all_images)} imagens para FID...")
        for img_name in tqdm(all_images[:num_samples*2], desc="Verificando imagens"):  # Check more than needed
            img_path = os.path.join(source_dir, img_name)
            try:
                with Image.open(img_path) as img:
                    img.convert('RGB')
                    img.load()
                    # Copy valid image to clean directory
                    shutil.copy2(img_path, os.path.join(output_dir, img_name))
                    valid_images.append(img_name)
                    if len(valid_images) >= num_samples:
                        break
            except Exception as e:
                print(f"Imagem corrompida ignorada: {img_name} - {e}")
                continue
        
        print(f"Copiadas {len(valid_images)} imagens v√°lidas para FID")
        return output_dir, len(valid_images)

def generate_and_save_images_for_fid(generator, latent_dim, num_attributes, num_images, device, output_path):
    """Gera e salva imagens para o c√°lculo do FID."""
    generator.eval() 
    imgs_saved = 0
    fid_batch_size = min(256, num_images)
    
    if os.path.exists(output_path):
        shutil.rmtree(output_path)
    os.makedirs(output_path)

    pbar_gen = tqdm(total=num_images, desc="Gerando imagens para FID")
    while imgs_saved < num_images:
        current_batch_size = min(num_images - imgs_saved, fid_batch_size)
        batch_noise = torch.randn(current_batch_size, latent_dim, device=device)
        batch_attrs = torch.randint(0, 2, (current_batch_size, num_attributes), dtype=torch.float32).to(device)

        with torch.no_grad():
            fake_images = generator(batch_noise, batch_attrs).cpu()

        for j in range(fake_images.size(0)):
            img = fake_images[j]
            img = img * 0.5 + 0.5
            save_image(img, os.path.join(output_path, f'generated_fid_{imgs_saved:05d}.png'))
            imgs_saved += 1
            pbar_gen.update(1)
            if imgs_saved >= num_images:
                break
    pbar_gen.close()
    generator.train()
    return output_path

def generate_image(generator, noise, attrs, device):
    """Gera uma √∫nica imagem dado ru√≠do e atributos."""
    generator.eval()
    with torch.no_grad():
        noise = noise.to(device)
        attrs = attrs.to(device)
        generated_image = generator(noise, attrs)
        generated_image = generated_image * 0.5 + 0.5
    generator.train()
    return generated_image.cpu()

# Inicializa√ß√£o das vari√°veis de treinamento
fid_scores_per_epoch = []
fid_epochs = []  # Track which epochs have FID scores

# Create clean real images directory for FID calculation
clean_real_dir = os.path.join('temp_clean_real_fid')
try:
    clean_real_path, num_valid_real = create_clean_real_images_for_fid(
        real_data_root, clean_real_dir, num_fid_samples
    )
    print(f"Preparado diret√≥rio limpo para FID com {num_valid_real} imagens v√°lidas")
    log_file.write(f"Preparado diret√≥rio limpo para FID com {num_valid_real} imagens v√°lidas\n")
except Exception as e:
    print(f"Erro ao preparar imagens para FID: {e}")
    log_file.write(f"Erro ao preparar imagens para FID: {e}\n")
    clean_real_path = real_data_root  # Fallback to original path

print("Iniciando o treinamento...")

fixed_noise = torch.randn(96, latent_dim, device=device)
fixed_attrs_base = torch.tensor([
    [1, 0, 0, 0, 0], 
    [0, 1, 0, 0, 0], 
    [1, 0, 1, 0, 0], 
    [0, 1, 1, 0, 0], 
    [1, 0, 0, 1, 0], 
    [0, 1, 0, 1, 0], 
    [1, 0, 0, 0, 1], 
    [0, 1, 0, 0, 1], 
    [0, 0, 0, 0, 0], 
    [1, 1, 0, 0, 0], 
    [0, 0, 1, 0, 0], 
    [1, 0, 0, 1, 1]  
], dtype=torch.float32).to(device)

fixed_attrs = fixed_attrs_base.repeat(8, 1) 

start_time = time.time()

best_g_loss = float('-inf')
best_epoch = 0

temp_gen_fid_dir = tempfile.mkdtemp(prefix='fid_gen_')

# Fun√ß√£o para formatar tempo
def format_time(seconds):
    m, s = divmod(seconds, 60)
    h, m = divmod(m, 60)
    return f"{int(h)}h {int(m)}m {int(s)}s"

# Loop principal de treinamento
for epoch in range(start_epoch, epochs + 1):
        epoch_start_time = time.time()

        pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f"Epoch {epoch}/{epochs}")

        current_d_loss_sum = 0
        current_g_loss_sum = 0
        g_steps_count = 0

        log_frequency = max(1, len(dataloader) // 5)

        for i, (real_images, attrs) in pbar:
            if i % log_frequency == 0:
                log_file.write(f"Epoch {epoch}/{epochs}, Batch {i}: real_images min: {real_images.min().item():.4f}, max: {real_images.max().item():.4f}\n")

            real_images = real_images.to(device, non_blocking=True)
            attrs = attrs.to(device, non_blocking=True)
            batch_size = real_images.size(0)

            netD.zero_grad()
            
            # Generate fake images for discriminator training
            noise = torch.randn(batch_size, latent_dim, device=device)
            
            # ***** AQUI: Autocast para D_loss (FP16) - Desabilitado por scaler=None *****
            # O uso de 'torch.amp.autocast' √© condicional a 'scaler is not None'.
            # Como 'scaler' est√° definido como 'None' para depura√ß√£o, tudo est√° rodando em FP32.
            if torch.cuda.is_available() and scaler is not None:
                with torch.amp.autocast('cuda'):
                    fake_images_for_d = netG(noise, attrs)
                    real_output = netD(real_images, attrs).view(-1)
                    fake_output = netD(fake_images_for_d.detach(), attrs).view(-1)
            else:
                # FP32 training without autocast
                fake_images_for_d = netG(noise, attrs)
                real_output = netD(real_images, attrs).view(-1)
                fake_output = netD(fake_images_for_d.detach(), attrs).view(-1)

            # Para o GP, precisamos que real_images e fake_images_for_d tenham requires_grad=True
            # Clonar e aplicar requires_grad_(True) √© a forma mais segura.
            real_images_for_gp = real_images.clone().detach().requires_grad_(True)
            fake_images_for_gp = fake_images_for_d.clone().detach().requires_grad_(True)

            # calculate_gradient_penalty √© chamado fora de autocast para garantir FP32 para GP
            gp = calculate_gradient_penalty(netD, real_images_for_gp, fake_images_for_gp, attrs, lambda_gp, device)
            
            if i % log_frequency == 0:
                log_file.write(f"Epoch {epoch}/{epochs}, Batch {i}: Gradient Penalty (GP): {gp.item():.4f}\n")

            d_loss = -torch.mean(real_output) + torch.mean(fake_output) + gp
            
            # Backward pass e otimiza√ß√£o
            if scaler: # Este bloco n√£o ser√° executado por enquanto, j√° que scaler √© None
                scaler.scale(d_loss).backward()
                scaler.unscale_(optimizerD)
                torch.nn.utils.clip_grad_norm_(netD.parameters(), max_norm=1.0)
                scaler.step(optimizerD)
                scaler.update()
            else: # Este bloco ser√° executado (FP32)
                d_loss.backward()
                torch.nn.utils.clip_grad_norm_(netD.parameters(), max_norm=1.0)
                optimizerD.step()
                
            current_d_loss_sum += d_loss.item()

            if i % d_steps == 0: # Treinar o Gerador
                netG.zero_grad()
                noise = torch.randn(batch_size, latent_dim, device=device)
                
                # ***** AQUI: Autocast para G_loss (FP16) - Desabilitado por scaler=None *****
                if torch.cuda.is_available() and scaler is not None:
                    with torch.amp.autocast('cuda'):
                        fake_images = netG(noise, attrs) # Gerador produz novas fake_images
                        g_output = netD(fake_images, attrs).view(-1)
                        g_loss = -torch.mean(g_output) # Gerador tenta maximizar a sa√≠da do D para fake_images
                else:
                    # FP32 training without autocast
                    fake_images = netG(noise, attrs) # Gerador produz novas fake_images
                    g_output = netD(fake_images, attrs).view(-1)
                    g_loss = -torch.mean(g_output) # Gerador tenta maximizar a sa√≠da do D para fake_images

                if torch.isnan(d_loss) or torch.isnan(g_loss):
                    log_file.write(f"ATEN√á√ÉO: NaN detectado na perda do D ou G na √âpoca {epoch}, Batch {i}. Interrompendo treinamento.\n")
                    print(f"ATEN√á√ÉO: NaN detectado na perda do D ou G na √âpoca {epoch}, Batch {i}. Interrompendo treinamento.")
                    break 
                
                # Check training stability
                is_stable = check_training_stability(d_loss.item(), g_loss.item(), epoch, i, log_file)
                if not is_stable and i > 100:  # Allow some initial instability
                    log_file.write(f"Training instability detected. Consider reducing learning rates or adjusting hyperparameters.\n")
                
                # Print detalhado das perdas para depura√ß√£o de NaNs
                if i % 10 == 0: 
                    # Note: real_output e fake_output aqui s√£o da √∫ltima itera√ß√£o do D, n√£o deste G
                    print(f"DEBUG Batch {i}: D_Loss={d_loss.item():.4f}, G_Loss={g_loss.item():.4f}, Real_Output_Mean={torch.mean(real_output).item():.4f}, Fake_Output_Mean_D_Side={torch.mean(fake_output).item():.4f}, GP={gp.item():.4f}, G_Output_G_Side={torch.mean(g_output).item():.4f}")
                    
                    # Enhanced debugging information
                    log_file.write(f"DETAILED DEBUG Epoch {epoch}, Batch {i}:\n")
                    log_file.write(f"  D_Loss: {d_loss.item():.4f}\n")
                    log_file.write(f"  G_Loss: {g_loss.item():.4f}\n")
                    log_file.write(f"  Gradient Penalty: {gp.item():.4f}\n")
                    log_file.write(f"  Real Output Stats: mean={torch.mean(real_output).item():.4f}, std={torch.std(real_output).item():.4f}\n")
                    log_file.write(f"  Fake Output Stats: mean={torch.mean(fake_output).item():.4f}, std={torch.std(fake_output).item():.4f}\n")
                    log_file.write(f"  G Output Stats: mean={torch.mean(g_output).item():.4f}, std={torch.std(g_output).item():.4f}\n")
                    
                if i % log_frequency == 0:
                    log_file.write(f"Epoch {epoch}/{epochs}, Batch {i}: G_Output (Discriminator on Fakes) Sample: {g_output[0].item():.4f}, Min: {g_output.min().item():.4f}, Max: {g_output.max().item():.4f}, Mean: {g_output.mean().item():.4f}\n")
                    log_file.write(f"Epoch {epoch}/{epochs}, Batch {i}: G_Loss calculated: {g_loss.item():.4f}\n")

                if scaler: # Este bloco n√£o ser√° executado por enquanto
                    scaler.scale(g_loss).backward()
                    scaler.unscale_(optimizerG)
                    torch.nn.utils.clip_grad_norm_(netG.parameters(), max_norm=1.0)
                    scaler.step(optimizerG)
                    scaler.update()
                else: # Este bloco ser√° executado (FP32)
                    g_loss.backward()
                    torch.nn.utils.clip_grad_norm_(netG.parameters(), max_norm=1.0)
                    optimizerG.step()
                
                current_g_loss_sum += g_loss.item()
                g_steps_count += 1

                if i % (log_frequency * 2) == 0:
                    log_file.write(f"Epoch {epoch}/{epochs}, Batch {i}: Generator Gradients (Norms):\n")
                    for name, param in netG.named_parameters():
                        if param.grad is not None:
                            norm = param.grad.norm().item()
                            log_file.write(f"  {name}: {norm:.6f}\n")
                            if norm > 1e6:
                                log_file.write(f"  WARNING: Exploding gradient detected for {name} in Generator!\n")
                            elif norm < 1e-8 and i > 100:
                                log_file.write(f"  WARNING: Vanishing gradient detected for {name} in Generator!\n")
            
            pbar.set_postfix({'Loss D': f'{d_loss.item():.4f}', 'Loss G': f'{g_loss.item():.4f}'})

        epoch_end_time = time.time()
        epoch_duration = epoch_end_time - epoch_start_time

        avg_d_loss = current_d_loss_sum / len(dataloader)
        avg_g_loss = current_g_loss_sum / g_steps_count if g_steps_count > 0 else 0

        g_losses_per_epoch.append(avg_g_loss)
        d_losses_per_epoch.append(avg_d_loss)

        total_time_elapsed = epoch_end_time - start_time
        avg_time_per_epoch = total_time_elapsed / epoch
        estimated_time_remaining = avg_time_per_epoch * (epochs - epoch)

        print(f"\n--- Epoch {epoch}/{epochs} Conclu√≠da ---")
        print(f"  Avg Loss D: {avg_d_loss:.4f} | Avg Loss G: {avg_g_loss:.4f}")
        print(f"  Dura√ß√£o da √âpoca: {format_time(epoch_duration)}")
        print(f"  Tempo Total Decorrido: {format_time(total_time_elapsed)}")
        print(f"  Tempo Estimado Restante: {format_time(estimated_time_remaining)}")
        print(f"  Progresso Total: {(epoch/epochs)*100:.2f}%")
        
        if epoch % 5 == 0:
            print_gpu_memory_usage()

        # Enhanced logging for debugging
        log_file.write(f"\n--- Epoch {epoch}/{epochs} Summary ---\n")
        log_file.write(f"  Discriminator - Avg Loss: {avg_d_loss:.4f}\n")
        log_file.write(f"  Generator - Avg Loss: {avg_g_loss:.4f}\n")
        log_file.write(f"  Generator Steps: {g_steps_count}\n")
        log_file.write(f"  Learning Rates - G: {optimizerG.param_groups[0]['lr']:.2e}, D: {optimizerD.param_groups[0]['lr']:.2e}\n")
        log_file.write(f"  Lambda GP: {lambda_gp}\n")
        log_file.write(f"  Dura√ß√£o da √âpoca: {format_time(epoch_duration)}\n")
        log_file.write(f"  Tempo Total Decorrido: {format_time(total_time_elapsed)}\n")
        log_file.write(f"  Tempo Estimado Restante: {format_time(estimated_time_remaining)}\n")
        log_file.write(f"  Progresso Total: {(epoch/epochs)*100:.2f}%\n")

        # Update learning rate schedulers
        schedulerG.step()
        schedulerD.step()

        with torch.no_grad():
            generated_samples = netG(fixed_noise, fixed_attrs).cpu()
            save_image(generated_samples, f'results/cgan_optimized/epoch_{epoch:03d}_generated_samples.png', nrow=12, normalize=True)

        if epoch % 5 == 0 or epoch == epochs:
            print("\nCalculando FID...")
            log_file.write("\nCalculando FID...\n")

            shutil.rmtree(temp_gen_fid_dir, ignore_errors=True)
            temp_gen_fid_dir = tempfile.mkdtemp(prefix='fid_gen_')

            gen_path = generate_and_save_images_for_fid(netG, latent_dim, num_attributes, num_fid_samples, device, temp_gen_fid_dir)

            try:
                fid_value = calculate_fid_given_paths([clean_real_path, gen_path],
                                                    batch_size,
                                                    device,
                                                    dims=2048)
                fid_scores_per_epoch.append(fid_value)
                fid_epochs.append(epoch)  # Track which epoch this FID score belongs to
                print(f"  FID Score na √âpoca {epoch}: {fid_value:.2f}")
                log_file.write(f"  FID Score na √âpoca {epoch}: {fid_value:.2f}\n")
                
                # Atualiza o melhor FID score
                if fid_value < best_fid_score:
                    best_fid_score = fid_value
                    print(f"  >>> Novo melhor FID Score: {best_fid_score:.2f} na √âpoca {epoch} <<<")
                    log_file.write(f"  >>> Novo melhor FID Score: {best_fid_score:.2f} na √âpoca {epoch} <<<\n")
                    
            except Exception as e:
                print(f"Erro ao calcular FID na √âpoca {epoch}: {e}")
                log_file.write(f"Erro ao calcular FID na √âpoca {epoch}: {e}\n")
                # Don't append anything to maintain consistency between fid_scores_per_epoch and fid_epochs

            shutil.rmtree(temp_gen_fid_dir)
            temp_gen_fid_dir = tempfile.mkdtemp(prefix='fid_gen_')

        if avg_g_loss > best_g_loss:
            best_g_loss = avg_g_loss
            best_epoch = epoch
            torch.save(netG.state_dict(), 'models/cgan_optimized/best_generator.pth')
            torch.save(netD.state_dict(), 'models/cgan_optimized/best_discriminator.pth')
            print(f"  >>> Modelo salvo! Nova melhor G Loss: {best_g_loss:.4f} na √âpoca {best_epoch} <<<")
            log_file.write(f"  >>> Modelo salvo! Nova melhor G Loss: {best_g_loss:.4f} na √âpoca {best_epoch} <<<\n")

        if epoch % 10 == 0:
            torch.save(netG.state_dict(), f'models/cgan_optimized/netG_epoch_{epoch:03d}.pth')
            torch.save(netD.state_dict(), f'models/cgan_optimized/netD_epoch_{epoch:03d}.pth')
        
        # Salva checkpoint conforme o intervalo configurado
        if epoch % args.checkpoint_interval == 0:
            save_checkpoint(epoch, netG, netD, optimizerG, optimizerD, g_losses_per_epoch, d_losses_per_epoch, best_fid_score, checkpoint_dir='checkpoints')

print("\nTreinamento conclu√≠do.")
log_file.write("\nTreinamento conclu√≠do.\n")

# Final summary logging
log_file.write(f"\n=== RESUMO FINAL DO TREINAMENTO ===\n")
log_file.write(f"Total de √©pocas completadas: {epochs}\n")
log_file.write(f"Melhor FID Score: {best_fid_score:.2f}\n")
log_file.write(f"Melhor Generator Loss: {best_g_loss:.4f} (√âpoca {best_epoch})\n")
log_file.write(f"Dura√ß√£o total: {format_time(time.time() - start_time)}\n")
log_file.write(f"Hyperparameters finais:\n")
log_file.write(f"  - Batch Size: {batch_size}\n")
log_file.write(f"  - Learning Rate G: {learning_rate_g}\n")
log_file.write(f"  - Learning Rate D: {learning_rate_d}\n")
log_file.write(f"  - Lambda GP: {lambda_gp}\n")
log_file.write(f"  - D Steps: {d_steps}\n")
log_file.write(f"=== FIM DO RESUMO ===\n")

log_file.close()

# Clean up temporary directories
try:
    if os.path.exists(clean_real_dir):
        shutil.rmtree(clean_real_dir)
    if os.path.exists(temp_gen_fid_dir):
        shutil.rmtree(temp_gen_fid_dir)
except Exception as e:
    print(f"Aviso: Erro ao limpar diret√≥rios tempor√°rios: {e}")

total_training_duration = time.time() - start_time
print(f"Dura√ß√£o total do treinamento: {format_time(total_training_duration)}")
print(f"Melhor FID Score alcan√ßado: {best_fid_score:.2f}")
print(f"Melhor Generator Loss: {best_g_loss:.4f} (√âpoca {best_epoch})")

# Plotting with proper error handling
try:
    plt.figure(figsize=(15, 5))
    
    # Loss plot
    plt.subplot(1, 2, 1)
    plt.plot(range(1, len(d_losses_per_epoch) + 1), d_losses_per_epoch, label='Discriminator Loss', alpha=0.7)
    plt.plot(range(1, len(g_losses_per_epoch) + 1), g_losses_per_epoch, label='Generator Loss', alpha=0.7)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('GAN Training Losses Over Epochs')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # FID plot (only if we have FID scores)
    if len(fid_scores_per_epoch) > 0 and len(fid_epochs) > 0:
        plt.subplot(1, 2, 2)
        # Filter out any NaN values
        valid_fid_data = [(epoch, fid) for epoch, fid in zip(fid_epochs, fid_scores_per_epoch) if not np.isnan(fid)]
        if valid_fid_data:
            valid_epochs, valid_fids = zip(*valid_fid_data)
            plt.plot(valid_epochs, valid_fids, marker='o', linestyle='-', color='red', label='FID Score')
            plt.xlabel('Epoch')
            plt.ylabel('FID Score')
            plt.title('FID Score Over Epochs')
            plt.legend()
            plt.grid(True, alpha=0.3)
        else:
            plt.subplot(1, 2, 2)
            plt.text(0.5, 0.5, 'Nenhum FID v√°lido calculado', ha='center', va='center', transform=plt.gca().transAxes)
            plt.title('FID Score Over Epochs')
    else:
        plt.subplot(1, 2, 2)
        plt.text(0.5, 0.5, 'FID n√£o calculado', ha='center', va='center', transform=plt.gca().transAxes)
        plt.title('FID Score Over Epochs')
    
    plt.tight_layout()
    plt.savefig('results/cgan_optimized/training_summary.png', dpi=150, bbox_inches='tight')
    plt.show()
    
except Exception as e:
    print(f"Erro ao gerar gr√°ficos: {e}")
    # Fallback: save individual plots
    try:
        plt.figure(figsize=(10, 5))
        plt.plot(range(1, len(d_losses_per_epoch) + 1), d_losses_per_epoch, label='Discriminator Loss')
        plt.plot(range(1, len(g_losses_per_epoch) + 1), g_losses_per_epoch, label='Generator Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('GAN Training Losses Over Epochs')
        plt.legend()
        plt.grid(True)
        plt.savefig('results/cgan_optimized/training_losses.png')
        plt.show()
        print("Gr√°fico de losses salvo com sucesso.")
    except Exception as e2:
        print(f"Erro ao salvar gr√°fico de losses: {e2}")

    # Exemplo de uso da fun√ß√£o generate_image (descomente para testar)
    # single_noise = torch.randn(1, latent_dim, device=device)
    # desired_attrs = torch.tensor([[1, 1, 1, 0, 0]], dtype=torch.float32).to(device) # Smiling, Male, Blond Hair
    # generated_single_image = generate_image(netG, single_noise, desired_attrs, device)
    # save_image(generated_single_image, 'results/cgan_optimized/single_generated_image.png')

    # plt.imshow(generated_single_image.squeeze().permute(1, 2, 0).numpy())
    # plt.title("Generated Image with Attributes: Smiling, Male, Blond Hair")
    # plt.axis('off')
    # plt.show()